{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01aH0PR4Sg-"
      },
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "outputs": [],
      "source": [
        "# code comment is used as inline annotations for your coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4WUjz64C3B"
      },
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(img_dir)\n",
        "cv2.imshow(\"Title\", img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [],
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  return None\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return None\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "  return None\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# modified from bart code: The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "\"\"\"PyTorch BART model, ported from the fairseq repo.\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import random, datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "# from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_callable\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx):\n",
        "    \"\"\" Replace non-padding symbols with their position numbers. Position numbers begin at\n",
        "    padding_idx+1. Padding symbols are ignored. This is modified from fairseq's\n",
        "    `utils.make_positions`.\n",
        "    :param torch.Tensor x:\n",
        "    :return torch.Tensor:\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
        "    return incremental_indices.long() + padding_idx\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/bart-large\",\n",
        "    \"facebook/bart-large-mnli\",\n",
        "    \"facebook/bart-large-cnn\",\n",
        "    \"facebook/bart-large-xsum\",\n",
        "    \"facebook/mbart-large-en-ro\",\n",
        "    # See all BART models at https://huggingface.co/models?filter=bart\n",
        "]\n",
        "\n",
        "\n",
        "BART_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class. Use it as a regular PyTorch Module and\n",
        "    refer to the PyTorch documentation for all matters related to general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
        "            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n",
        "\n",
        "\"\"\"\n",
        "BART_GENERATION_EXAMPLE = r\"\"\"\n",
        "    Examples::\n",
        "\n",
        "        from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "        # see ``examples/summarization/bart/evaluate_cnn.py`` for a longer example\n",
        "        model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
        "        tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
        "        ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "        inputs = tokenizer.batch_encode_plus([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
        "        # Generate Summary\n",
        "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
        "        print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "               Indices of input sequence tokens in the vocabulary. Use BartTokenizer.encode to produce them.\n",
        "            Padding will be ignored by default should you provide it.\n",
        "            Indices can be obtained using :class:`transformers.BartTokenizer.encode(text)`.\n",
        "        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Mask to avoid performing attention on padding token indices in input_ids.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`, defaults to :obj:`None`):\n",
        "            Tuple consists of (`last_hidden_state`, `optional`: `hidden_states`, `optional`: `attentions`)\n",
        "            `last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`, defaults to :obj:`None`) is a sequence of hidden-states at the output of the last layer of the encoder.\n",
        "            Used in the cross-attention of the decoder.\n",
        "        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Provide for translation and summarization training. By default, the model will create this tensor by shifting the input_ids right, following the paper.\n",
        "        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`, defaults to :obj:`None`):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in decoder_input_ids. Causal mask will also be used by default.\n",
        "            If you want to change padding behavior, you should read :func:`~transformers.modeling_bart._prepare_decoder_inputs` and modify.\n",
        "            See diagram 1 in the paper for more info on the default strategy\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def invert_mask(attention_mask):\n",
        "    assert attention_mask.dim() == 2\n",
        "    return attention_mask.eq(0)\n",
        "\n",
        "def _prepare_bart_decoder_inputs(\n",
        "    config, input_ids, decoder_input_ids=None, decoder_padding_mask=None, causal_mask_dtype=torch.float32\n",
        "):\n",
        "    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n",
        "    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n",
        "    Note: this is not called during generation\n",
        "    \"\"\"\n",
        "    pad_token_id = config.pad_token_id\n",
        "    if decoder_input_ids is None:\n",
        "        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n",
        "    bsz, tgt_len = decoder_input_ids.size()\n",
        "    if decoder_padding_mask is None:\n",
        "        decoder_padding_mask = make_padding_mask(decoder_input_ids, pad_token_id)\n",
        "    else:\n",
        "        decoder_padding_mask = invert_mask(decoder_padding_mask)\n",
        "    causal_mask = torch.triu(fill_with_neg_inf(torch.zeros(tgt_len, tgt_len)), 1).to(\n",
        "        dtype=causal_mask_dtype, device=decoder_input_ids.device\n",
        "    )\n",
        "    return decoder_input_ids, decoder_padding_mask, causal_mask\n",
        "\n",
        "\n",
        "class PretrainedBartModel(PreTrainedModel):\n",
        "    config_class = BartConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs\n",
        "\n",
        "\n",
        "def _make_linear_from_emb(emb):\n",
        "    vocab_size, emb_size = emb.weight.shape\n",
        "    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "    lin_layer.weight.data = emb.weight.data\n",
        "    return lin_layer\n",
        "\n",
        "\n",
        "# Helper Functions, mostly for making masks\n",
        "def _check_shapes(shape_1, shape2):\n",
        "    if shape_1 != shape2:\n",
        "        raise AssertionError(\"shape mismatch: {} != {}\".format(shape_1, shape2))\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
        "    prev_output_tokens = input_ids.clone()\n",
        "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "    return prev_output_tokens\n",
        "\n",
        "\n",
        "def make_padding_mask(input_ids, padding_idx=1):\n",
        "    \"\"\"True for pad tokens\"\"\"\n",
        "    padding_mask = input_ids.eq(padding_idx)\n",
        "    if not padding_mask.any():\n",
        "        padding_mask = None\n",
        "    return padding_mask\n",
        "\n",
        "\n",
        "# Helper Modules\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.self_attn = SelfAttention(\n",
        "            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.normalize_before = config.normalize_before\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
        "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
        "            for t_tgt, t_src is excluded (or masked out), =0 means it is\n",
        "            included in attention\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        x, attn_weights = self.self_attn(\n",
        "            query=x, key=x, key_padding_mask=encoder_padding_mask, need_weights=self.output_attentions\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class BartEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n",
        "    is a :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        embed_dim = embed_tokens.embedding_dim\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, embed_dim, self.padding_idx\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.max_position_embeddings, embed_dim, self.padding_idx,\n",
        "            )\n",
        "        if config.date_visit_embeddings:\n",
        "            self.embed_visitids = DateYearMonthDayEmbedding(config.max_position_embeddings, embed_dim, self.padding_idx)\n",
        "        else:\n",
        "            self.embed_visitids = nn.Embedding(1460, embed_dim) \n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = LayerNorm(embed_dim) if config.normalize_embedding else nn.Identity()\n",
        "        # mbart has one extra layer_norm\n",
        "        self.layer_norm = LayerNorm(config.d_model) if config.normalize_before else None\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask=None, visit_ids=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (LongTensor): tokens in the source language of shape\n",
        "                `(batch, src_len)`\n",
        "            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n",
        "        Returns:\n",
        "            Tuple comprised of:\n",
        "                - **x** (Tensor): the last encoder layer's output of\n",
        "                  shape `(src_len, batch, embed_dim)`\n",
        "                - **encoder_states** (List[Tensor]): all intermediate\n",
        "                  hidden states of shape `(src_len, batch, embed_dim)`.\n",
        "                  Only populated if *self.output_hidden_states:* is True.\n",
        "                - **all_attentions** (List[Tensor]): Attention weights for each layer.\n",
        "                During training might not be of length n_layers because of layer dropout.\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = invert_mask(attention_mask)\n",
        "\n",
        "        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        embed_pos = self.embed_positions(input_ids)\n",
        "        embed_visit = self.embed_visitids(visit_ids)\n",
        "        x = inputs_embeds + embed_pos + embed_visit\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        encoder_states, all_attentions = [], []\n",
        "        for encoder_layer in self.layers:\n",
        "            if self.output_hidden_states:\n",
        "                encoder_states.append(x)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                attn = None\n",
        "            else:\n",
        "                x, attn = encoder_layer(x, attention_mask)\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions.append(attn)\n",
        "\n",
        "        if self.layer_norm:\n",
        "            x = self.layer_norm(x)\n",
        "        if self.output_hidden_states:\n",
        "            encoder_states.append(x)\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        encoder_states = [hidden_state.transpose(0, 1) for hidden_state in encoder_states]\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, encoder_states, all_attentions\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.self_attn = SelfAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.normalize_before = config.normalize_before\n",
        "\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = SelfAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attn_mask=None,\n",
        "        layer_state=None,\n",
        "        causal_mask=None,\n",
        "        decoder_padding_mask=None,\n",
        "    ):\n",
        "        residual = x\n",
        "\n",
        "        if layer_state is None:\n",
        "            layer_state = {}\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        # Self Attention\n",
        "\n",
        "        x, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            layer_state=layer_state,  # adds keys to layer state\n",
        "            key_padding_mask=decoder_padding_mask,\n",
        "            attn_mask=causal_mask,\n",
        "            need_weights=self.output_attentions,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        # Cross attention\n",
        "        residual = x\n",
        "        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n",
        "        if self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "        x, _ = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_hidden_states,\n",
        "            key_padding_mask=encoder_attn_mask,\n",
        "            layer_state=layer_state,  # mutates layer state\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            layer_state,\n",
        "        )  # just self_attn weights for now, following t5, layer_state = cache for decoding\n",
        "\n",
        "\n",
        "class BartDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n",
        "    is a :class:`DecoderLayer`.\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, config.pad_token_id\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, self.padding_idx,\n",
        "            )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderLayer(config) for _ in range(config.decoder_layers)]\n",
        "        )  # type: List[DecoderLayer]\n",
        "        self.layernorm_embedding = LayerNorm(config.d_model) if config.normalize_embedding else nn.Identity()\n",
        "        self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_padding_mask,\n",
        "        decoder_padding_mask,\n",
        "        decoder_causal_mask,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "        **unused\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Includes several features from \"Jointly Learning to Align and\n",
        "        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n",
        "\n",
        "        Args:\n",
        "            input_ids (LongTensor): previous decoder outputs of shape\n",
        "                `(batch, tgt_len)`, for teacher forcing\n",
        "            encoder_hidden_states: output from the encoder, used for\n",
        "                encoder-side attention\n",
        "            encoder_padding_mask: for ignoring pad tokens\n",
        "            decoder_cached_states (dict or None): dictionary used for storing state during generation\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
        "                - hidden states\n",
        "                - attentions\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if encoder_padding_mask is not None:\n",
        "            encoder_padding_mask = invert_mask(encoder_padding_mask)\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_ids, use_cache=use_cache)\n",
        "\n",
        "        if use_cache:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "            positions = positions[:, -1:]  # happens after we embed them\n",
        "            # assert input_ids.ne(self.padding_idx).any()\n",
        "\n",
        "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        x += positions\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = ()\n",
        "        all_self_attns = ()\n",
        "        next_decoder_cache = []\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states += (x,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            layer_state = decoder_cached_states[idx] if decoder_cached_states is not None else None\n",
        "\n",
        "            x, layer_self_attn, layer_past = decoder_layer(\n",
        "                x,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attn_mask=encoder_padding_mask,\n",
        "                decoder_padding_mask=decoder_padding_mask,\n",
        "                layer_state=layer_state,\n",
        "                causal_mask=decoder_causal_mask,\n",
        "            )\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache.append(layer_past.copy())\n",
        "\n",
        "            if self.layer_norm and (idx == len(self.layers) - 1):  # last layer of mbart\n",
        "                x = self.layer_norm(x)\n",
        "            if self.output_attentions:\n",
        "                all_self_attns += (layer_self_attn,)\n",
        "\n",
        "        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        all_hidden_states = [hidden_state.transpose(0, 1) for hidden_state in all_hidden_states]\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        if use_cache:\n",
        "            next_cache = ((encoder_hidden_states, encoder_padding_mask), next_decoder_cache)\n",
        "        else:\n",
        "            next_cache = None\n",
        "        return x, next_cache, all_hidden_states, list(all_self_attns)\n",
        "\n",
        "\n",
        "def _reorder_buffer(attn_cache, new_order):\n",
        "    for k, input_buffer_k in attn_cache.items():\n",
        "        if input_buffer_k is not None:\n",
        "            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n",
        "    return attn_cache\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n",
        "\n",
        "    def _shape(self, tensor, dim_0, bsz):\n",
        "        return tensor.contiguous().view(dim_0, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query,\n",
        "        key: Optional[Tensor],\n",
        "        key_padding_mask: Optional[Tensor] = None,\n",
        "        layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "        need_weights=False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n",
        "        static_kv: bool = self.encoder_decoder_attention\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        # get here for encoder decoder cause of static_kv\n",
        "        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n",
        "            saved_state = layer_state.get(self.cache_key, {})\n",
        "            if \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute key and value if they are static\n",
        "                if static_kv:\n",
        "                    key = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "            layer_state = {}\n",
        "\n",
        "        q = self.q_proj(query) * self.scaling\n",
        "        if static_kv:\n",
        "            if key is None:\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self._shape(q, tgt_len, bsz)\n",
        "        if k is not None:\n",
        "            k = self._shape(k, -1, bsz)\n",
        "        if v is not None:\n",
        "            v = self._shape(v, -1, bsz)\n",
        "\n",
        "        if saved_state is not None:\n",
        "            k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)\n",
        "\n",
        "        # Update cache\n",
        "        layer_state[self.cache_key] = {\n",
        "            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_key_padding_mask\": key_padding_mask if not static_kv else None,\n",
        "        }\n",
        "\n",
        "        assert k is not None\n",
        "        src_len = k.size(1)\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attn_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "        assert key_padding_mask is None or key_padding_mask.size()[:2] == (bsz, src_len,)\n",
        "\n",
        "        if key_padding_mask is not None:  # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training,)\n",
        "\n",
        "        assert v is not None\n",
        "        attn_output = torch.bmm(attn_probs, v)\n",
        "        assert attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights = None\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv, bsz):\n",
        "        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "        if \"prev_key\" in saved_state:\n",
        "            _prev_key = saved_state[\"prev_key\"]\n",
        "            assert _prev_key is not None\n",
        "            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                k = prev_key\n",
        "            else:\n",
        "                assert k is not None\n",
        "                k = torch.cat([prev_key, k], dim=1)\n",
        "        if \"prev_value\" in saved_state:\n",
        "            _prev_value = saved_state[\"prev_value\"]\n",
        "            assert _prev_value is not None\n",
        "            prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                v = prev_value\n",
        "            else:\n",
        "                assert v is not None\n",
        "                v = torch.cat([prev_value, v], dim=1)\n",
        "        assert k is not None and v is not None\n",
        "        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\"prev_key_padding_mask\", None)\n",
        "        key_padding_mask = self._cat_prev_key_padding_mask(\n",
        "            key_padding_mask, prev_key_padding_mask, bsz, k.size(1), static_kv\n",
        "        )\n",
        "        return k, v, key_padding_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_prev_key_padding_mask(\n",
        "        key_padding_mask: Optional[Tensor],\n",
        "        prev_key_padding_mask: Optional[Tensor],\n",
        "        batch_size: int,\n",
        "        src_len: int,\n",
        "        static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None:\n",
        "            if static_kv:\n",
        "                new_key_padding_mask = prev_key_padding_mask\n",
        "            else:\n",
        "                new_key_padding_mask = torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)\n",
        "\n",
        "        elif key_padding_mask is not None:\n",
        "            filler = torch.zeros(\n",
        "                batch_size,\n",
        "                src_len - key_padding_mask.size(1),\n",
        "                dtype=key_padding_mask.dtype,\n",
        "                device=key_padding_mask.device,\n",
        "            )\n",
        "            new_key_padding_mask = torch.cat([filler, key_padding_mask], dim=1)\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "\n",
        "class ICDBartForPreTraining(PretrainedBartModel):\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        base_model = BartModel(config)\n",
        "        self.model = base_model\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        old_num_tokens = self.model.shared.num_embeddings\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self.model.shared = new_embeddings\n",
        "        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        lm_labels=None,\n",
        "        use_cache=False,\n",
        "        visit_ids=None,\n",
        "        **unused\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        lm_labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n",
        "            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n",
        "            with labels\n",
        "            in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:\n",
        "        masked_lm_loss (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Masked language modeling loss.\n",
        "        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`)\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "            # Mask filling only works for bart-large\n",
        "            from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "            tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
        "            TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
        "            model = BartForConditionalGeneration.from_pretrained('bart-large')\n",
        "            input_ids = tokenizer.batch_encode_plus([TXT], return_tensors='pt')['input_ids']\n",
        "            logits = model(input_ids)[0]\n",
        "            masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "            probs = logits[0, masked_index].softmax(dim=0)\n",
        "            values, predictions = probs.topk(5)\n",
        "            tokenizer.decode(predictions).split()\n",
        "            # ['good', 'great', 'all', 'really', 'very']\n",
        "        \"\"\"\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "            visit_ids=visit_ids,\n",
        "        )\n",
        "        lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)\n",
        "        outputs = (lm_logits,) + outputs[1:]  # Add cache, hidden states and attention if they are here\n",
        "        if lm_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index = -100)\n",
        "            # TODO(SS): do we need to ignore pad tokens in lm_labels?\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), lm_labels.view(-1))\n",
        "            outputs = (masked_lm_loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def prepare_inputs_for_generation(self, decoder_input_ids, past, attention_mask, use_cache, **kwargs):\n",
        "        assert past is not None, \"past has to be defined for encoder_outputs\"\n",
        "\n",
        "        # first step, decoder_cached_states are empty\n",
        "        if not past[1]:\n",
        "            encoder_outputs, decoder_cached_states = past, None\n",
        "        else:\n",
        "            encoder_outputs, decoder_cached_states = past\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"decoder_cached_states\": decoder_cached_states,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_logits_for_generation(self, logits, cur_len, max_length):\n",
        "        if cur_len == 1:\n",
        "            self._force_token_ids_generation(logits, self.config.bos_token_id)\n",
        "        if cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
        "            self._force_token_ids_generation(logits, self.config.eos_token_id)\n",
        "        return logits\n",
        "\n",
        "    def _force_token_ids_generation(self, scores, token_ids) -> None:\n",
        "        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\n",
        "        if isinstance(token_ids, int):\n",
        "            token_ids = [token_ids]\n",
        "        all_but_token_ids_mask = torch.tensor(\n",
        "            [x for x in range(self.config.vocab_size) if x not in token_ids],\n",
        "            dtype=torch.long,\n",
        "            device=next(self.parameters()).device,\n",
        "        )\n",
        "        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n",
        "        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        ((enc_out, enc_mask), decoder_cached_states) = past\n",
        "        reordered_past = []\n",
        "        for layer_past in decoder_cached_states:\n",
        "            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n",
        "            layer_past_new = {\n",
        "                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n",
        "            }\n",
        "            reordered_past.append(layer_past_new)\n",
        "\n",
        "        new_enc_out = enc_out if enc_out is None else enc_out.index_select(0, beam_idx)\n",
        "        new_enc_mask = enc_mask if enc_mask is None else enc_mask.index_select(0, beam_idx)\n",
        "\n",
        "        past = ((new_enc_out, new_enc_mask), reordered_past)\n",
        "        return past\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.encoder\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##   Training\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * train() method of Train Class will be used to train the model, \n",
        "  * AdamW optimizer will be used in get_optimizers(), \n",
        "  * evaluate() methode will be used for evaluation,\n",
        "  * predict() method will be used to predict output, \n",
        "  * auc_metrics() method will be used to evaluate the model.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from contextlib import contextmanager\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, List, Optional, Tuple\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "import torch\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from transformers.data.data_collator import DataCollator\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "#from transformers.optimization import AdamW, WarmupLinearSchedule\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, EvalPrediction, PredictionOutput, TrainOutput\n",
        "from transformers.training_args import TrainingArguments\n",
        "#from transformers.default_data_collator import DataCollator\n",
        "\n",
        "def is_tpu_available():\n",
        "    return False\n",
        "\n",
        "try:\n",
        "    from apex import amp\n",
        "\n",
        "    _has_apex = True\n",
        "except ImportError:\n",
        "    _has_apex = False\n",
        "\n",
        "TEST_ID_STRS = [\n",
        "    'chronic_PTSD_0',\n",
        "    'type_2_diabtes_0',\n",
        "    'hyperlipidemia_0',\n",
        "    'loin_pain_0',\n",
        "    'low_back_pain_0',\n",
        "    'PTSD_0',\n",
        "    'obstructive_sleep_apnea_hypopnea_0',\n",
        "    'mental_depression_0',\n",
        "    'chronic_obstructive_airway_disease_0',\n",
        "    'sensorineural_hearing_loss_0',\n",
        "    'gastroesophagel_reflux_disease_without_esophagitis_0',\n",
        "    'gastroesophagel_reflux_disease_0',\n",
        "    'coronary_arteriosclerosis_0',\n",
        "    'arteriosclerotic_heart_disease_0',\n",
        "    'chronic_PTSD_3',\n",
        "    'type_2_diabtes_3',\n",
        "    'hyperlipidemia_3',\n",
        "    'loin_pain_3',\n",
        "    'low_back_pain_3',\n",
        "    'PTSD_3',\n",
        "    'obstructive_sleep_apnea_hypopnea_3',\n",
        "    'mental_depression_3',\n",
        "    'chronic_obstructive_airway_disease_3',\n",
        "    'sensorineural_hearing_loss_3',\n",
        "    'gastroesophagel_reflux_disease_without_esophagitis_3',\n",
        "    'gastroesophagel_reflux_disease_3',\n",
        "    'coronary_arteriosclerosis_3',\n",
        "    'arteriosclerotic_heart_disease_3',\n",
        "    'chronic_PTSD_6',\n",
        "    'type_2_diabtes_6',\n",
        "    'hyperlipidemia_6',\n",
        "    'loin_pain_6',\n",
        "    'low_back_pain_6',\n",
        "    'PTSD_6',\n",
        "    'obstructive_sleep_apnea_hypopnea_6',\n",
        "    'mental_depression_6',\n",
        "    'chronic_obstructive_airway_disease_6',\n",
        "    'sensorineural_hearing_loss_6',\n",
        "    'gastroesophagel_reflux_disease_without_esophagitis_6',\n",
        "    'gastroesophagel_reflux_disease_6',\n",
        "    'coronary_arteriosclerosis_6',\n",
        "    'arteriosclerotic_heart_disease_6',\n",
        "    'least_happen'\n",
        "]\n",
        "\n",
        "def is_apex_available():\n",
        "    return _has_apex\n",
        "\n",
        "\n",
        "if is_tpu_available():\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.debug.metrics as met\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "    _has_tensorboard = True\n",
        "except ImportError:\n",
        "    try:\n",
        "        from tensorboardX import SummaryWriter\n",
        "\n",
        "        _has_tensorboard = True\n",
        "    except ImportError:\n",
        "        _has_tensorboard = False\n",
        "\n",
        "\n",
        "def is_tensorboard_available():\n",
        "    return _has_tensorboard\n",
        "\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "\n",
        "    wandb.ensure_configured()\n",
        "    if wandb.api.api_key is None:\n",
        "        _has_wandb = False\n",
        "        wandb.termwarn(\"W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\")\n",
        "    else:\n",
        "        _has_wandb = False if os.getenv(\"WANDB_DISABLED\") else True\n",
        "except ImportError:\n",
        "    _has_wandb = False\n",
        "\n",
        "\n",
        "def is_wandb_available():\n",
        "    return _has_wandb\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # ^^ safe to call this function even if cuda is not available\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def torch_distributed_zero_first(local_rank: int):\n",
        "    \"\"\"\n",
        "    Decorator to make all processes in distributed training wait for each local_master to do something.\n",
        "    \"\"\"\n",
        "    if local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "    yield\n",
        "    if local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "\n",
        "class SequentialDistributedSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Distributed Sampler that subsamples indicies sequentially,\n",
        "    making it easier to collate all results at the end.\n",
        "\n",
        "    Even though we only use this sampler for eval and predict (no training),\n",
        "    which means that the model params won't have to be synced (i.e. will not hang\n",
        "    for synchronization even if varied number of forward passes), we still add extra\n",
        "    samples to the sampler to make it evenly divisible (like in `DistributedSampler`)\n",
        "    to make it easy to `gather` or `reduce` resulting tensors at the end of the loop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, num_replicas=None, rank=None):\n",
        "        if num_replicas is None:\n",
        "            if not torch.distributed.is_available():\n",
        "                raise RuntimeError(\"Requires distributed package to be available\")\n",
        "            num_replicas = torch.distributed.get_world_size()\n",
        "        if rank is None:\n",
        "            if not torch.distributed.is_available():\n",
        "                raise RuntimeError(\"Requires distributed package to be available\")\n",
        "            rank = torch.distributed.get_rank()\n",
        "        self.dataset = dataset\n",
        "        self.num_replicas = num_replicas\n",
        "        self.rank = rank\n",
        "        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n",
        "        self.total_size = self.num_samples * self.num_replicas\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.dataset)))\n",
        "\n",
        "        # add extra samples to make it evenly divisible\n",
        "        indices += indices[: (self.total_size - len(indices))]\n",
        "        assert len(indices) == self.total_size\n",
        "\n",
        "        # subsample\n",
        "        indices = indices[self.rank * self.num_samples : (self.rank + 1) * self.num_samples]\n",
        "        assert len(indices) == self.num_samples\n",
        "\n",
        "        return iter(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "def get_tpu_sampler(dataset: Dataset):\n",
        "    if xm.xrt_world_size() <= 1:\n",
        "        return RandomSampler(dataset)\n",
        "    return DistributedSampler(dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal())\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Trainer is a simple but feature-complete training and eval loop for PyTorch,\n",
        "    optimized for Transformers.\n",
        "    \"\"\"\n",
        "\n",
        "    model: PreTrainedModel\n",
        "    args: TrainingArguments\n",
        "    data_collator: DataCollator\n",
        "    train_dataset: Optional[Dataset]\n",
        "    eval_dataset: Optional[Dataset]\n",
        "    test_datasets: Optional[List[Dataset]]\n",
        "    test_collator: Optional[DataCollator]\n",
        "    compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None\n",
        "    prediction_loss_only: bool\n",
        "    tb_writer: Optional[\"SummaryWriter\"] = None\n",
        "    optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None\n",
        "    global_step: Optional[int] = None\n",
        "    epoch: Optional[float] = None\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        args: TrainingArguments,\n",
        "        data_collator: Optional[DataCollator] = None,\n",
        "        train_dataset: Optional[Dataset] = None,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        test_datasets: Optional[List[Dataset]] = None,\n",
        "        test_collator: Optional[DataCollator] = None,\n",
        "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
        "        prediction_loss_only=False,\n",
        "        tb_writer: Optional[\"SummaryWriter\"] = None,\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Trainer is a simple but feature-complete training and eval loop for PyTorch,\n",
        "        optimized for Transformers.\n",
        "\n",
        "        Args:\n",
        "            prediction_loss_only:\n",
        "                (Optional) in evaluation and prediction, only return the loss\n",
        "        \"\"\"\n",
        "        self.model = model.to(args.device)\n",
        "        self.args = args\n",
        "        if data_collator is not None:\n",
        "            self.data_collator = data_collator\n",
        "        else:\n",
        "            self.data_collator = DefaultDataCollator()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.test_datasets = test_datasets\n",
        "        if test_collator is not None:\n",
        "            self.test_collator = test_collator\n",
        "        else:\n",
        "            self.test_collator = DefaultDataCollator()\n",
        "        self.compute_metrics = compute_metrics\n",
        "        self.prediction_loss_only = prediction_loss_only\n",
        "        self.optimizers = optimizers\n",
        "        if tb_writer is not None:\n",
        "            self.tb_writer = tb_writer\n",
        "        elif is_tensorboard_available() and self.is_world_master():\n",
        "            self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)\n",
        "        if not is_tensorboard_available():\n",
        "            logger.warning(\n",
        "                \"You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\"\n",
        "            )\n",
        "        if is_wandb_available():\n",
        "            self._setup_wandb()\n",
        "        else:\n",
        "            logger.info(\n",
        "                \"You are instantiating a Trainer but W&B is not installed. To use wandb logging, \"\n",
        "                \"run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\"\n",
        "            )\n",
        "        set_seed(self.args.seed)\n",
        "        # Create output directory if needed\n",
        "        if self.is_world_master():\n",
        "            os.makedirs(self.args.output_dir, exist_ok=True)\n",
        "        if is_tpu_available():\n",
        "            # Set an xla_device flag on the model's config.\n",
        "            # We'll find a more elegant and not need to do this in the future.\n",
        "            self.model.config.xla_device = True\n",
        "\n",
        "    def get_train_dataloader(self) -> DataLoader:\n",
        "        if self.train_dataset is None:\n",
        "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
        "        if is_tpu_available():\n",
        "            train_sampler = get_tpu_sampler(self.train_dataset)\n",
        "        else:\n",
        "            train_sampler = (\n",
        "                RandomSampler(self.train_dataset)\n",
        "                if self.args.local_rank == -1\n",
        "                else DistributedSampler(self.train_dataset)\n",
        "            )\n",
        "\n",
        "        data_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.train_batch_size,\n",
        "            sampler=train_sampler,\n",
        "            collate_fn=self.data_collator.collate_batch,\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
        "        if eval_dataset is None and self.eval_dataset is None:\n",
        "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
        "\n",
        "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "\n",
        "        if is_tpu_available():\n",
        "            sampler = SequentialDistributedSampler(\n",
        "                eval_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n",
        "            )\n",
        "        elif self.args.local_rank != -1:\n",
        "            sampler = SequentialDistributedSampler(eval_dataset)\n",
        "        else:\n",
        "            sampler = SequentialSampler(eval_dataset)\n",
        "\n",
        "        data_loader = DataLoader(\n",
        "            eval_dataset,\n",
        "            sampler=sampler,\n",
        "            batch_size=self.args.eval_batch_size,\n",
        "            collate_fn=self.data_collator.collate_batch,\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def get_test_dataloaders(self, test_datasets: Optional[List[Dataset]] = None) -> List[DataLoader]:\n",
        "        if test_datasets is None and self.test_datasets is None:\n",
        "            raise ValueError(\"Trainer: evaluation requires an test_datasets.\")\n",
        "\n",
        "        test_datasets = test_datasets if test_datasets is not None else self.test_datasets\n",
        "\n",
        "        data_loaders = []\n",
        "        for test_dataset in test_datasets:\n",
        "            if is_tpu_available():\n",
        "                sampler = SequentialDistributedSampler(\n",
        "                    test_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n",
        "                )\n",
        "            elif self.args.local_rank != -1:\n",
        "                sampler = SequentialDistributedSampler(test_dataset)\n",
        "            else:\n",
        "                sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "            data_loader = DataLoader(\n",
        "                test_dataset,\n",
        "                sampler=sampler,\n",
        "                batch_size=self.args.eval_batch_size,\n",
        "                collate_fn=self.test_collator.collate_batch,\n",
        "            )\n",
        "\n",
        "            data_loaders.append(data_loader)\n",
        "\n",
        "        return data_loaders\n",
        "\n",
        "    def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n",
        "        # We use the same batch_size as for eval.\n",
        "        if is_tpu_available():\n",
        "            sampler = SequentialDistributedSampler(\n",
        "                test_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n",
        "            )\n",
        "        elif self.args.local_rank != -1:\n",
        "            sampler = SequentialDistributedSampler(test_dataset)\n",
        "        else:\n",
        "            sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "        data_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler=sampler,\n",
        "            batch_size=self.args.eval_batch_size,\n",
        "            collate_fn=self.data_collator.collate_batch,\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def get_optimizers(\n",
        "        self, num_training_steps: int\n",
        "    ) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n",
        "        \"\"\"\n",
        "        Setup the optimizer and the learning rate scheduler.\n",
        "\n",
        "        We provide a reasonable default that works well.\n",
        "        If you want to use something else, you can pass a tuple in the Trainer's init,\n",
        "        or override this method in a subclass.\n",
        "        \"\"\"\n",
        "        if self.optimizers is not None:\n",
        "            return self.optimizers\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.args.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
        "        )\n",
        "        return optimizer, scheduler\n",
        "\n",
        "    def _setup_wandb(self):\n",
        "        \"\"\"\n",
        "        Setup the optional Weights & Biases (`wandb`) integration.\n",
        "\n",
        "        One can override this method to customize the setup if needed.  Find more information at https://docs.wandb.com/huggingface\n",
        "        You can also override the following environment variables:\n",
        "\n",
        "        Environment:\n",
        "            WANDB_WATCH:\n",
        "                (Optional, [\"gradients\", \"all\", \"false\"]) \"gradients\" by default, set to \"false\" to disable gradient logging\n",
        "                or \"all\" to log gradients and parameters\n",
        "            WANDB_PROJECT:\n",
        "                (Optional): str - \"huggingface\" by default, set this to a custom string to store results in a different project\n",
        "            WANDB_DISABLED:\n",
        "                (Optional): boolean - defaults to false, set to \"true\" to disable wandb entirely\n",
        "        \"\"\"\n",
        "        logger.info('Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"')\n",
        "        wandb.init(project=os.getenv(\"WANDB_PROJECT\", \"huggingface\"), config=vars(self.args))\n",
        "        # keep track of model topology and gradients\n",
        "        if os.getenv(\"WANDB_WATCH\") != \"false\":\n",
        "            wandb.watch(\n",
        "                self.model, log=os.getenv(\"WANDB_WATCH\", \"gradients\"), log_freq=max(100, self.args.logging_steps)\n",
        "            )\n",
        "\n",
        "    def num_examples(self, dataloader: DataLoader) -> int:\n",
        "        \"\"\"\n",
        "        Helper to get num of examples from a DataLoader, by accessing its Dataset.\n",
        "        \"\"\"\n",
        "        return len(dataloader.dataset)\n",
        "\n",
        "    def train(self, model_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Main training entry point.\n",
        "\n",
        "        Args:\n",
        "            model_path:\n",
        "                (Optional) Local path to model if model to train has been instantiated from a local path\n",
        "                If present, we will try reloading the optimizer/scheduler states from there.\n",
        "        \"\"\"\n",
        "        train_dataloader = self.get_train_dataloader()\n",
        "        if self.args.max_steps > 0:\n",
        "            t_total = self.args.max_steps\n",
        "            num_train_epochs = (\n",
        "                self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
        "            )\n",
        "        else:\n",
        "            t_total = int(len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs)\n",
        "            num_train_epochs = self.args.num_train_epochs\n",
        "\n",
        "        optimizer, scheduler = self.get_optimizers(num_training_steps=t_total)\n",
        "\n",
        "        # Check if saved optimizer or scheduler states exist\n",
        "        if (\n",
        "            model_path is not None\n",
        "            and os.path.isfile(os.path.join(model_path, \"optimizer.pt\"))\n",
        "            and os.path.isfile(os.path.join(model_path, \"scheduler.pt\"))\n",
        "        ):\n",
        "            # Load in optimizer and scheduler states\n",
        "            optimizer.load_state_dict(\n",
        "                torch.load(os.path.join(model_path, \"optimizer.pt\"), map_location=self.args.device)\n",
        "            )\n",
        "            scheduler.load_state_dict(torch.load(os.path.join(model_path, \"scheduler.pt\")))\n",
        "\n",
        "        model = self.model\n",
        "        if self.args.fp16:\n",
        "            if not is_apex_available():\n",
        "                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "            model, optimizer = amp.initialize(model, optimizer, opt_level=self.args.fp16_opt_level)\n",
        "\n",
        "        # multi-gpu training (should be after apex fp16 initialization)\n",
        "        if self.args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        # Distributed training (should be after apex fp16 initialization)\n",
        "        if self.args.local_rank != -1:\n",
        "            model = torch.nn.parallel.DistributedDataParallel(\n",
        "                model,\n",
        "                device_ids=[self.args.local_rank],\n",
        "                output_device=self.args.local_rank,\n",
        "                find_unused_parameters=True,\n",
        "            )\n",
        "\n",
        "        if self.tb_writer is not None:\n",
        "            self.tb_writer.add_text(\"args\", self.args.to_json_string())\n",
        "            self.tb_writer.add_hparams(self.args.to_sanitized_dict(), metric_dict={})\n",
        "\n",
        "        # Train!\n",
        "        total_train_batch_size = (\n",
        "            self.args.train_batch_size\n",
        "            * self.args.gradient_accumulation_steps\n",
        "            * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)\n",
        "        )\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", self.num_examples(train_dataloader))\n",
        "        logger.info(\"  Num Epochs = %d\", num_train_epochs)\n",
        "        logger.info(\"  Instantaneous batch size per device = %d\", self.args.per_device_train_batch_size)\n",
        "        logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\", total_train_batch_size)\n",
        "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
        "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.epoch = 0\n",
        "        epochs_trained = 0\n",
        "        steps_trained_in_current_epoch = 0\n",
        "        # Check if continuing training from a checkpoint\n",
        "        if model_path is not None:\n",
        "            # set global_step to global_step of last saved checkpoint from model path\n",
        "            try:\n",
        "                self.global_step = int(model_path.split(\"-\")[-1].split(\"/\")[0])\n",
        "                epochs_trained = self.global_step // (len(train_dataloader) // self.args.gradient_accumulation_steps)\n",
        "                steps_trained_in_current_epoch = self.global_step % (\n",
        "                    len(train_dataloader) // self.args.gradient_accumulation_steps\n",
        "                )\n",
        "\n",
        "                logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "                logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "                logger.info(\"  Continuing training from global step %d\", self.global_step)\n",
        "                logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "            except ValueError:\n",
        "                self.global_step = 0\n",
        "                logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "        tr_loss = 0.0\n",
        "        logging_loss = 0.0\n",
        "        model.zero_grad()\n",
        "        train_iterator = trange(\n",
        "            epochs_trained, int(num_train_epochs), desc=\"Epoch\", disable=not self.is_local_master(), mininterval=3600\n",
        "        )\n",
        "        for epoch in train_iterator:\n",
        "            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n",
        "                train_dataloader.sampler.set_epoch(epoch)\n",
        "\n",
        "\n",
        "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=not self.is_local_master(), mininterval=600)\n",
        "\n",
        "            for step, inputs in enumerate(epoch_iterator):\n",
        "\n",
        "                # Skip past any already trained steps if resuming training\n",
        "                if steps_trained_in_current_epoch > 0:\n",
        "                    steps_trained_in_current_epoch -= 1\n",
        "                    continue\n",
        "\n",
        "                tr_loss += self._training_step(model, inputs, optimizer)\n",
        "\n",
        "                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n",
        "                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
        "                    len(epoch_iterator) <= self.args.gradient_accumulation_steps\n",
        "                    and (step + 1) == len(epoch_iterator)\n",
        "                ):\n",
        "                    if self.args.fp16:\n",
        "                        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), self.args.max_grad_norm)\n",
        "                    else:\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    scheduler.step()\n",
        "                    model.zero_grad()\n",
        "                    self.global_step += 1\n",
        "                    self.epoch = epoch + (step + 1) / len(epoch_iterator)\n",
        "\n",
        "                    if (self.args.logging_steps > 0 and self.global_step % self.args.logging_steps == 0) or (\n",
        "                        self.global_step == 1 and self.args.logging_first_step\n",
        "                    ):\n",
        "                        logs: Dict[str, float] = {}\n",
        "                        if self.global_step == 1 and self.args.logging_first_step:\n",
        "                            logs[\"loss\"] = (tr_loss - logging_loss)\n",
        "                        else:\n",
        "                            logs[\"loss\"] = (tr_loss - logging_loss) / self.args.logging_steps\n",
        "                        # backward compatibility for pytorch schedulers\n",
        "                        logs[\"learning_rate\"] = (\n",
        "                            scheduler.get_last_lr()[0]\n",
        "                            if version.parse(torch.__version__) >= version.parse(\"1.4\")\n",
        "                            else scheduler.get_lr()[0]\n",
        "                        )\n",
        "                        logging_loss = tr_loss\n",
        "\n",
        "                        self._log(logs)\n",
        "\n",
        "                        # if self.args.evaluate_during_training:\n",
        "                        self.evaluate()\n",
        "\n",
        "                    if self.args.save_steps > 0 and self.global_step % self.args.save_steps == 0:\n",
        "                        # In all cases (even distributed/parallel), self.model is always a reference\n",
        "                        # to the model we want to save.\n",
        "                        if hasattr(model, \"module\"):\n",
        "                            assert model.module is self.model\n",
        "                        else:\n",
        "                            assert model is self.model\n",
        "                        # Save model checkpoint\n",
        "                        output_dir = os.path.join(self.args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
        "\n",
        "                        self.save_model(output_dir)\n",
        "\n",
        "                        if self.is_world_master():\n",
        "                            self._rotate_checkpoints()\n",
        "\n",
        "                        if is_tpu_available():\n",
        "                            xm.rendezvous(\"saving_optimizer_states\")\n",
        "                            xm.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                            xm.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        elif self.is_world_master():\n",
        "                            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "\n",
        "                if self.args.max_steps > 0 and self.global_step > self.args.max_steps:\n",
        "                    epoch_iterator.close()\n",
        "                    break\n",
        "            if self.args.max_steps > 0 and self.global_step > self.args.max_steps:\n",
        "                train_iterator.close()\n",
        "                break\n",
        "            if self.args.tpu_metrics_debug:\n",
        "                # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
        "                xm.master_print(met.metrics_report())\n",
        "\n",
        "        if self.tb_writer:\n",
        "            self.tb_writer.close()\n",
        "\n",
        "        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
        "        return TrainOutput(self.global_step, tr_loss / self.global_step, None)\n",
        "\n",
        "    def _log(self, logs: Dict[str, float], iterator: Optional[tqdm] = None) -> None:\n",
        "        if self.epoch is not None:\n",
        "            logs[\"epoch\"] = self.epoch\n",
        "        if self.tb_writer:\n",
        "            for k, v in logs.items():\n",
        "                self.tb_writer.add_scalar(k, v, self.global_step)\n",
        "        if is_wandb_available():\n",
        "            wandb.log(logs, step=self.global_step)\n",
        "        output = json.dumps({**logs, **{\"step\": self.global_step}})\n",
        "        if iterator is not None:\n",
        "            iterator.write(output)\n",
        "        else:\n",
        "            print(output)\n",
        "\n",
        "    def _training_step(\n",
        "        self, model: nn.Module, inputs: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer\n",
        "    ) -> float:\n",
        "        model.train()\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(self.args.device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "        if self.args.n_gpu > 1:\n",
        "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "        if self.args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "        if self.args.fp16:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def is_local_master(self) -> bool:\n",
        "        if is_tpu_available():\n",
        "            return xm.is_master_ordinal(local=True)\n",
        "        else:\n",
        "            return self.args.local_rank in [-1, 0]\n",
        "\n",
        "    def is_world_master(self) -> bool:\n",
        "        \"\"\"\n",
        "        This will be True only in one process, even in distributed mode,\n",
        "        even when training on multiple machines.\n",
        "        \"\"\"\n",
        "        if is_tpu_available():\n",
        "            return xm.is_master_ordinal(local=False)\n",
        "        else:\n",
        "            return self.args.local_rank == -1 or torch.distributed.get_rank() == 0\n",
        "\n",
        "    def save_model(self, output_dir: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Saving best-practices: if you use default names for the model,\n",
        "        you can reload it using from_pretrained().\n",
        "\n",
        "        Will only save from the world_master process (unless in TPUs).\n",
        "        \"\"\"\n",
        "\n",
        "        if is_tpu_available():\n",
        "            self._save_tpu(output_dir)\n",
        "        elif self.is_world_master():\n",
        "            self._save(output_dir)\n",
        "\n",
        "    def _save_tpu(self, output_dir: Optional[str] = None):\n",
        "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
        "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        if xm.is_master_ordinal():\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Save a trained model and configuration using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        if not isinstance(self.model, PreTrainedModel):\n",
        "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
        "\n",
        "        xm.rendezvous(\"saving_checkpoint\")\n",
        "        self.model.save_pretrained(output_dir)\n",
        "\n",
        "    def _save(self, output_dir: Optional[str] = None):\n",
        "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "        # Save a trained model and configuration using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        if not isinstance(self.model, PreTrainedModel):\n",
        "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
        "        self.model.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "\n",
        "    def _sorted_checkpoints(self, checkpoint_prefix=PREFIX_CHECKPOINT_DIR, use_mtime=False) -> List[str]:\n",
        "        ordering_and_checkpoint_path = []\n",
        "\n",
        "        glob_checkpoints = [str(x) for x in Path(self.args.output_dir).glob(f\"{checkpoint_prefix}-*\")]\n",
        "\n",
        "        for path in glob_checkpoints:\n",
        "            if use_mtime:\n",
        "                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "            else:\n",
        "                regex_match = re.match(f\".*{checkpoint_prefix}-([0-9]+)\", path)\n",
        "                if regex_match and regex_match.groups():\n",
        "                    ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "        checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "        return checkpoints_sorted\n",
        "\n",
        "    def _rotate_checkpoints(self, use_mtime=False) -> None:\n",
        "        if self.args.save_total_limit is None or self.args.save_total_limit <= 0:\n",
        "            return\n",
        "\n",
        "        # Check if we should delete older checkpoint(s)\n",
        "        checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime)\n",
        "        if len(checkpoints_sorted) <= self.args.save_total_limit:\n",
        "            return\n",
        "\n",
        "        number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - self.args.save_total_limit)\n",
        "        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "        for checkpoint in checkpoints_to_be_deleted:\n",
        "            logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "            shutil.rmtree(checkpoint)\n",
        "\n",
        "    def evaluate(\n",
        "        self, eval_dataset: Optional[Dataset] = None, prediction_loss_only: Optional[bool] = None,\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Run evaluation and return metrics.\n",
        "\n",
        "        The calling script will be responsible for providing a method to compute metrics, as they are\n",
        "        task-dependent.\n",
        "\n",
        "        Args:\n",
        "            eval_dataset: (Optional) Pass a dataset if you wish to override\n",
        "            the one on the instance.\n",
        "        Returns:\n",
        "            A dict containing:\n",
        "                - the eval loss\n",
        "                - the potential metrics computed from the predictions\n",
        "        \"\"\"\n",
        "        if self.test_datasets is not None:\n",
        "            test_dataloaders = self.get_test_dataloaders()\n",
        "            for idx, test_dataloader in enumerate(test_dataloaders):\n",
        "                output = self._prediction_loop(test_dataloader, description=\"%02d_%s\"%(idx+1,TEST_ID_STRS[idx]), is_test=True) \n",
        "                self._log(output.metrics)\n",
        "\n",
        "\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self._prediction_loop(eval_dataloader, description=\"00_valid\")\n",
        "        self._log(output.metrics)\n",
        "\n",
        "        if self.args.tpu_metrics_debug:\n",
        "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
        "            xm.master_print(met.metrics_report())\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "    def predict(self, test_dataset: Dataset) -> PredictionOutput:\n",
        "        \"\"\"\n",
        "        Run prediction and return predictions and potential metrics.\n",
        "\n",
        "        Depending on the dataset and your use case, your test dataset may contain labels.\n",
        "        In that case, this method will also return metrics, like in evaluate().\n",
        "        \"\"\"\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "\n",
        "        return self._prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def _prediction_loop(\n",
        "        self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None, is_test = False\n",
        "    ) -> PredictionOutput:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by `evaluate()` and `predict()`.\n",
        "\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        save_act_predict_result= True\n",
        "        save_count=0\n",
        "\n",
        "        def calc_overlap_todel(logits:torch.Tensor, labels:torch.Tensor, device=None) -> torch.Tensor:\n",
        "            num_batch = logits.shape[0] #(batch, toks, vocab_size)\n",
        "            assert num_batch == labels.shape[0] # check same number of batch (batch, toks)\n",
        "            result = 0.0\n",
        "            for i in range(num_batch):\n",
        "                label = labels[i][labels[i]!=-100]\n",
        "                logit = logits[i][labels[i]!=-100]\n",
        "                pred = torch.argmax(logit.detach(), axis=1)\n",
        "\n",
        "                label = label.detach().cpu().numpy().tolist()\n",
        "                label = set(label)\n",
        "                pred = pred.detach().cpu().numpy().tolist()\n",
        "                pred = set(pred)\n",
        "                overlap = pred.intersection(label)\n",
        "                joint = pred.union(label)\n",
        "                # result += len(overlap)/float(len(label))\n",
        "                result += len(overlap)/float(len(joint)) #TODO: change to joint\n",
        "            return torch.tensor(result, dtype=torch.float, device=device) \n",
        "\n",
        "        def calc_overlap_bart_todel(logits:torch.Tensor, labels:torch.Tensor, device=None) -> torch.Tensor:\n",
        "            num_batch = logits.shape[0] #(batch, toks, vocab_size)\n",
        "            assert num_batch == labels.shape[0] # check same number of batch (batch, toks)\n",
        "            result = 0.0\n",
        "            pred_len = []\n",
        "            actu_len = []\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                label = labels[i][labels[i]!=-100]\n",
        "                logit = logits[i][labels[i]!=-100]\n",
        "                pred = torch.argmax(logit.detach(), axis=1)\n",
        "\n",
        "\n",
        "                tmp = label[0]\n",
        "                assert tmp>200 and tmp <300\n",
        "                actu_len.append(tmp.item())\n",
        "                label = label[1:]\n",
        "\n",
        "                tmp = pred[0]\n",
        "                pred_len.append(tmp.item())\n",
        "                pred = pred[1:]\n",
        "\n",
        "                label = label.detach().cpu().numpy().tolist()\n",
        "                label = set(label)\n",
        "                pred  = pred.detach().cpu().numpy().tolist()\n",
        "                pred  = set(pred)\n",
        "                overlap = pred.intersection(label)\n",
        "                joint = pred.union(label)\n",
        "                # result += len(overlap)/float(len(label))\n",
        "                result += len(overlap)/float(len(joint)) #TODO: change to joint\n",
        "                # roc_auc = auc_metrics(yhat_raw, y, ymic)\n",
        "            pred_len = np.array(pred_len)\n",
        "            actu_len = np.array(actu_len)\n",
        "            return torch.tensor(result, dtype=torch.float, device=device), torch.tensor(pred_len==actu_len, dtype=torch.float, device=device).sum()\n",
        "\n",
        "        def auc_metrics(yhat_raw, y, ymic):\n",
        "            if yhat_raw.shape[0] <= 1:\n",
        "                return\n",
        "            fpr = {}\n",
        "            tpr = {}\n",
        "            roc_auc = {}\n",
        "            #get AUC for each label individually\n",
        "            relevant_labels = []\n",
        "            auc_labels = {}\n",
        "            for i in range(y.shape[1]):\n",
        "                #only if there are true positives for this label\n",
        "                if y[:,i].sum() > 0:\n",
        "                    fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])\n",
        "                    if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
        "                        auc_score = auc(fpr[i], tpr[i])\n",
        "                        if not np.isnan(auc_score): \n",
        "                            auc_labels[\"auc_%d\" % i] = auc_score\n",
        "                            relevant_labels.append(i)\n",
        "\n",
        "            #macro-AUC: just average the auc scores\n",
        "            aucs = []\n",
        "            for i in relevant_labels:\n",
        "                aucs.append(auc_labels['auc_%d' % i])\n",
        "            roc_auc['auc_macro'] = np.mean(aucs)\n",
        "\n",
        "            #micro-AUC: just look at each individual prediction\n",
        "            yhatmic = yhat_raw.ravel()\n",
        "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic) \n",
        "            roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "            return roc_auc\n",
        "\n",
        "        def calc_overlap(logits:torch.Tensor, labels:torch.Tensor, device=None) -> torch.Tensor:\n",
        "            num_batch = logits.shape[0] #(batch, toks, vocab_size)\n",
        "            vocab_size = logits.shape[2] #(batch, toks, vocab_size)\n",
        "            assert num_batch == labels.shape[0] # check same number of batch (batch, toks)\n",
        "            result = 0.0\n",
        "            for i in range(num_batch):\n",
        "                label = labels[i][labels[i]!=-100]\n",
        "                logit = logits[i][labels[i]!=-100]\n",
        "                pred = torch.argmax(logit.detach(), axis=1)\n",
        "\n",
        "                label = label.detach().cpu().numpy().tolist()\n",
        "                label = set(label)\n",
        "                pred = pred.detach().cpu().numpy().tolist()\n",
        "                pred = set(pred)\n",
        "                overlap = pred.intersection(label)\n",
        "                joint = pred.union(label)\n",
        "                # result += len(overlap)/float(len(label))\n",
        "                result += len(overlap)/float(len(joint)) #TODO: change to joint\n",
        "            return torch.tensor(result, dtype=torch.float, device=device) \n",
        "\n",
        "        def calc_overlap_bart(logits:torch.Tensor, labels:torch.Tensor, device=None) -> torch.Tensor:\n",
        "            num_batch = logits.shape[0] #(batch, toks, vocab_size)\n",
        "            vocab_size = logits.shape[2] #(batch, toks, vocab_size)\n",
        "            assert num_batch == labels.shape[0] # check same number of batch (batch, toks)\n",
        "            result = 0.0\n",
        "            pred_len = [1]\n",
        "            actu_len = [1]\n",
        "\n",
        "            yhat_raw = []\n",
        "            y = []\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                label = labels[i][labels[i]!=-100]\n",
        "                logit = logits[i][labels[i]!=-100]\n",
        "\n",
        "                pred = torch.argmax(logit.detach(), axis=1)\n",
        "                yhat_raw_tmpa, _ = torch.min(logit, axis=0)\n",
        "                countaa = 1\n",
        "                for a in pred[1:]:\n",
        "                    yhat_raw_tmpa[a] = logit[countaa][a]\n",
        "                    countaa += 1\n",
        "                yhat_raw.append(yhat_raw_tmpa.detach().tolist())\n",
        "\n",
        "                y_tmpa = np.zeros(vocab_size)\n",
        "                for a in label[1:]:\n",
        "                    y_tmpa[a] = 1\n",
        "                y.append(y_tmpa)\n",
        "            yhat_raw = np.array(yhat_raw)\n",
        "            y = np.array(y)\n",
        "            ymic = y.ravel()\n",
        "            \n",
        "            roc_auc = auc_metrics(yhat_raw, y, ymic)\n",
        "            pred_len = np.array(pred_len)\n",
        "            actu_len = np.array(actu_len)\n",
        "            return torch.tensor(roc_auc[\"auc_macro\"], dtype=torch.float, device=device), torch.tensor(roc_auc[\"auc_micro\"], dtype=torch.float, device=device).sum()\n",
        "\n",
        "        model = self.model\n",
        "        # multi-gpu eval\n",
        "        if self.args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "        else:\n",
        "            model = self.model\n",
        "        # Note: in torch.distributed mode, there's no point in wrapping the model\n",
        "        # inside a DistributedDataParallel as we'll be under `no_grad` anyways.\n",
        "\n",
        "        batch_size = dataloader.batch_size\n",
        "        logger.info(\"***** Running %s *****\", description)\n",
        "        logger.info(\"  Num examples = %d\", self.num_examples(dataloader))\n",
        "        logger.info(\"  Batch size = %d\", batch_size)\n",
        "        eval_losses: List[float] = []\n",
        "        preds: torch.Tensor = None\n",
        "        label_ids: torch.Tensor = None\n",
        "        accuracy: torch.Tensor = None\n",
        "        num_accuracy: torch.Tensor = None\n",
        "        meanap: torch.Tensor = None\n",
        "        num_meanap: torch.Tensor = None\n",
        "        meanlen : torch.Tensor = None\n",
        "        model.eval()\n",
        "\n",
        "        if is_tpu_available():\n",
        "            dataloader = pl.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)\n",
        "\n",
        "        for inputs in tqdm(dataloader, desc=description, mininterval=1200):\n",
        "            has_labels = any(inputs.get(k) is not None for k in [\"labels\", \"lm_labels\", \"masked_lm_labels\"])\n",
        "\n",
        "            for k, v in inputs.items():\n",
        "                inputs[k] = v.to(self.args.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                if has_labels:\n",
        "                    step_eval_loss, logits = outputs[:2]\n",
        "                    eval_losses += [step_eval_loss.mean().item()]\n",
        "                else:\n",
        "                    logits = outputs[0]\n",
        "\n",
        "            if not prediction_loss_only:\n",
        "                if inputs.get(\"labels\") is not None:\n",
        "                    labels = inputs[\"labels\"].detach()\n",
        "                elif inputs.get(\"masked_lm_labels\") is not None:\n",
        "                    labels = inputs['masked_lm_labels'].detach()\n",
        "                elif inputs.get(\"lm_labels\") is not None:\n",
        "                    labels = inputs['lm_labels'].detach()\n",
        "                else:\n",
        "                    print('Warning: no label found')\n",
        "\n",
        "                if is_test:\n",
        "                    if self.args.load_cui_leng and (not self.args.no_length_planning): \n",
        "                        mmap_score_sum, mlen_score_sum = calc_overlap_bart(logits.detach(), labels, device=self.args.device)\n",
        "                        if meanap is None:\n",
        "                            meanap = mmap_score_sum\n",
        "                        else:\n",
        "                            meanap += mmap_score_sum\n",
        "                        if num_meanap is None:\n",
        "                            num_meanap = torch.tensor(1, dtype=torch.long, device=self.args.device)\n",
        "                        else:\n",
        "                            num_meanap += torch.tensor(1, dtype=torch.long, device=self.args.device)\n",
        "                        if meanlen is None:\n",
        "                            meanlen = mlen_score_sum\n",
        "                        else:\n",
        "                            meanlen += mlen_score_sum\n",
        "                    else:\n",
        "                        mmap_score_sum, mlen_score_sum = calc_overlap_bart(logits.detach(), labels, device=self.args.device)\n",
        "                        if meanap is None:\n",
        "                            meanap = mmap_score_sum\n",
        "                        else:\n",
        "                            meanap += mmap_score_sum\n",
        "                        if num_meanap is None:\n",
        "                            num_meanap = torch.tensor(1, dtype=torch.long, device=self.args.device)\n",
        "                        else:\n",
        "                            num_meanap += torch.tensor(1, dtype=torch.long, device=self.args.device)\n",
        "                        if meanlen is None:\n",
        "                            meanlen = mlen_score_sum\n",
        "                        else:\n",
        "                            meanlen += mlen_score_sum\n",
        "\n",
        "                preds = logits[labels!=-100]\n",
        "                preds = torch.argmax(preds.detach(), axis=1)\n",
        "                label_ids = labels[labels!=-100]\n",
        "                if (preds is not None) and (label_ids is not None):\n",
        "                    if accuracy is None:\n",
        "                        accuracy = (preds==label_ids).sum()\n",
        "                    else:\n",
        "                        accuracy += (preds==label_ids).sum()\n",
        "                    if num_accuracy is None:\n",
        "                        num_accuracy = torch.tensor(label_ids.shape[0], dtype=torch.long, device=self.args.device) # number of masked tok\n",
        "                    else:\n",
        "                        num_accuracy += torch.tensor(label_ids.shape[0], dtype=torch.long, device=self.args.device)\n",
        "\n",
        "                if self.args.local_rank < 1 and save_act_predict_result and save_count < 101:\n",
        "                    save_folder = os.path.join(self.args.output_dir, \"eval_results\")\n",
        "                    save_folder = os.path.join(save_folder, \"eval_results_{}\".format(self.global_step))\n",
        "                    if not os.path.exists(save_folder):\n",
        "                        os.makedirs(save_folder)\n",
        "                    save_path = os.path.join(save_folder, \"{}.txt\".format(description))\n",
        "                    with open(save_path, \"a\") as writer: \n",
        "                        for i in range(labels.shape[0]):\n",
        "                            writer.write(\"-----------\\n\")\n",
        "                            writer.write(\"idx {}\\n\".format(save_count))\n",
        "                            writer.write(str(inputs[\"input_ids\"][i]))\n",
        "                            writer.write(\"\\n\")\n",
        "                            writer.write(str(labels[i]))\n",
        "                            writer.write(\"\\n\")\n",
        "                            if inputs.get(\"visit_ids\") is not None:\n",
        "                                writer.write(str(inputs[\"visit_ids\"][i]))\n",
        "                                writer.write(\"\\n\")\n",
        "                            pred = logits[i][labels[i]!=-100]\n",
        "                            if pred.shape[0] > 0:\n",
        "                                pred = torch.argmax(pred.detach(), axis=1)\n",
        "                                writer.write(str(pred))\n",
        "                                writer.write(\"\\n\")\n",
        "                                label_id = labels[i][labels[i]!=-100]\n",
        "                                writer.write(str(label_id))\n",
        "                                writer.write(\"\\n\")\n",
        "                            save_count += 1\n",
        "        \n",
        "        if accuracy is not None:\n",
        "            accuracy = torch.unsqueeze(accuracy,0)\n",
        "            num_accuracy = torch.unsqueeze(num_accuracy,0)\n",
        "        if meanap is not None:\n",
        "            meanap = torch.unsqueeze(meanap,0)\n",
        "            num_meanap = torch.unsqueeze(num_meanap,0)\n",
        "        if meanlen is not None:\n",
        "            meanlen = torch.unsqueeze(meanlen,0)\n",
        "\n",
        "\n",
        "        if self.args.local_rank != -1:\n",
        "            # In distributed mode, concatenate all results from all nodes:\n",
        "            if accuracy is not None:\n",
        "                accuracy = self.distributed_concat(accuracy, num_total_examples=self.num_examples(dataloader))\n",
        "            if num_accuracy is not None:\n",
        "                num_accuracy = self.distributed_concat(num_accuracy, num_total_examples=self.num_examples(dataloader))\n",
        "            if meanap is not None:\n",
        "                meanap = self.distributed_concat(meanap, num_total_examples=self.num_examples(dataloader))\n",
        "            if num_meanap is not None:\n",
        "                num_meanap = self.distributed_concat(num_meanap, num_total_examples=self.num_examples(dataloader))\n",
        "            if meanlen is not None:\n",
        "                meanlen = self.distributed_concat(meanlen, num_total_examples=self.num_examples(dataloader))\n",
        "        elif is_tpu_available():\n",
        "            # tpu-comment: Get all predictions and labels from all worker shards of eval dataset\n",
        "            if accuracy is not None:\n",
        "                accuracy = xm.mesh_reduce(\"eval_preds\", accuracy, torch.cat)\n",
        "            if num_accuracy is not None:\n",
        "                num_accuracy = xm.mesh_reduce(\"eval_label_ids\", num_accuracy, torch.cat)\n",
        "\n",
        "        # Finally, turn the aggregated tensors into numpy arrays.\n",
        "        if accuracy is not None:\n",
        "            accuracy = accuracy.cpu().numpy()\n",
        "        if num_accuracy is not None:\n",
        "            num_accuracy = num_accuracy.cpu().numpy()\n",
        "        if meanap is not None:\n",
        "            meanap = meanap.cpu().numpy()\n",
        "        if num_meanap is not None:\n",
        "            num_meanap = num_meanap.cpu().numpy()\n",
        "        if meanlen is not None:\n",
        "            meanlen = meanlen.cpu().numpy()\n",
        "\n",
        "        metrics = {}\n",
        "        if accuracy is not None and num_accuracy is not None:\n",
        "            acc = accuracy.sum() / float(num_accuracy.sum())\n",
        "            metrics['mask_acc'] = acc\n",
        "        if is_test:\n",
        "            if meanap is not None and num_meanap is not None:\n",
        "                mean_ap = meanap / num_meanap\n",
        "                metrics['map'] = float(mean_ap[0])\n",
        "            if meanlen is not None and num_meanap is not None:\n",
        "                mean_cuilength_in_last_visit = meanlen / num_meanap\n",
        "                metrics['mean_cuilengthcorrect'] = float(mean_cuilength_in_last_visit[0])  \n",
        "        \n",
        "            \n",
        "        if len(eval_losses) > 0:\n",
        "            metrics[f\"eval_{description}_loss\"] = np.mean(eval_losses)\n",
        "\n",
        "        # Prefix all keys with eval_ and description \n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(\"eval\"):\n",
        "                metrics[f\"eval_{description}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics)\n",
        "\n",
        "    def distributed_concat(self, tensor: torch.Tensor, num_total_examples: int) -> torch.Tensor:\n",
        "        assert self.args.local_rank != -1\n",
        "\n",
        "        output_tensors = [tensor.clone() for _ in range(torch.distributed.get_world_size())]\n",
        "        torch.distributed.all_gather(output_tensors, tensor)\n",
        "\n",
        "        concat = torch.cat(output_tensors, dim=0)\n",
        "\n",
        "        # truncate the dummy elements added by SequentialDistributedSampler\n",
        "        output = concat[:num_total_examples]\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (HfArgumentParser, BertTokenizer, AutoConfig, AutoModelWithLMHead,\n",
        "                          EncoderDecoderModel, Trainer, TrainingArguments)\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Define the argument dataclasses\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: str = 'bert-base-uncased'\n",
        "    config_name: str = None\n",
        "    tokenizer_name: str = 'bert-base-uncased'\n",
        "    cache_dir: str = './model_cache'\n",
        "    model_type: str = 'bert'\n",
        "    icd_training: bool = True\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    eval_data_file: str = None\n",
        "    block_size: int = -1\n",
        "    mlm: bool = True\n",
        "    mlm_probability: float = 0.15\n",
        "    do_pos_emb: bool = True\n",
        "    load_cui_leng: bool = False\n",
        "    no_length_planning: bool = False\n",
        "\n",
        "# Parse the arguments (simulated here, usually you would use command-line arguments)\n",
        "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "model_args, data_args, training_args = parser.parse_args_into_dataclasses([\n",
        "    '--output_dir', './results',\n",
        "    '--num_train_epochs', '3',\n",
        "    '--per_device_train_batch_size', '16',\n",
        "    '--save_steps', '1000',\n",
        "    '--seed', '42',\n",
        "    '--do_train',\n",
        "    '--do_eval',\n",
        "    '--evaluation_strategy', 'epoch',\n",
        "    '--overwrite_output_dir',\n",
        "    '--warmup_steps', '500',\n",
        "    '--logging_dir', './logs',\n",
        "    '--logging_steps', '10'\n",
        "])\n",
        "\n",
        "# Additional setup\n",
        "if data_args.eval_data_file is None and training_args.do_eval:\n",
        "    raise ValueError(\"Cannot do evaluation without an evaluation data file.\")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "               training_args.local_rank, training_args.device, training_args.n_gpu,\n",
        "               bool(training_args.local_rank != -1), training_args.fp16)\n",
        "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "if os.path.exists(training_args.output_dir) and os.listdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
        "    raise ValueError(f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\")\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(training_args.seed)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
        "config = AutoConfig.from_pretrained(model_args.model_name_or_path if model_args.config_name is None else model_args.config_name, cache_dir=model_args.cache_dir)\n",
        "\n",
        "# Decide on the model type\n",
        "if model_args.icd_training:\n",
        "    model = ICDBartForPreTraining.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelWithLMHead.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "if data_args.block_size <= 0:\n",
        "    data_args.block_size = tokenizer.max_len\n",
        "    # Our input block size will be the max possible for the model\n",
        "else:\n",
        "    data_args.block_size = data_args.block_size\n",
        "# Get datasets\n",
        "print('model_args', model_args)\n",
        "print('data_args', data_args)\n",
        "print('training_args', training_args)\n",
        "print('tokenizer', tokenizer)\n",
        "from dataset import DataCollatorForICDBERT, DataCollatorForICDBERTFINALPRED, DataCollatorForICDBART, prepare_dataset\n",
        "train_dataset, eval_dataset, test_datasets = prepare_dataset(model_args, data_args, training_args, tokenizer=tokenizer)\n",
        "# train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "# eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
        "if data_args.load_cui_leng:\n",
        "    data_collator = DataCollatorForICDBART(\n",
        "        tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability, no_length_planning=data_args.no_length_planning,\n",
        "    )\n",
        "    test_data_collator = DataCollatorForICDBART(\n",
        "        tokenizer=tokenizer, mlm=True, mlm_probability=-0.1, decode_targets_masked_only=True, no_length_planning=data_args.no_length_planning\n",
        "    )\n",
        "else:\n",
        "    if model_args.do_multi:\n",
        "        data_collator = DataCollatorForICDBERTFINALPRED(\n",
        "            tokenizer=tokenizer, numlastvisit=1, do_pos_emb=data_args.do_pos_emb, for_bert=False\n",
        "        )\n",
        "    else:\n",
        "        data_collator = DataCollatorForICDBERT(\n",
        "            tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability, do_pos_emb=data_args.do_pos_emb, for_bert=False\n",
        "        ) \n",
        "    test_data_collator = DataCollatorForICDBERTFINALPRED(\n",
        "        tokenizer=tokenizer, numlastvisit=1, do_pos_emb=data_args.do_pos_emb, for_bert=False\n",
        "    )\n",
        "\n",
        "# print(\"len(train_dataset)\")\n",
        "# print(len(train_dataset))\n",
        "# print(\"len(eval_dataset)\")\n",
        "# print(len(eval_dataset))\n",
        "# def compute_metrics(p:EvalPrediction)-> Dict:\n",
        "#     return {\"acc\": simple_accuracy(p.predictions, p.label_ids)}\n",
        "# Initialize our Trainer\n",
        "training_args.load_cui_leng = data_args.load_cui_leng\n",
        "training_args.no_length_planning = data_args.no_length_planning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    test_collator=test_data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    test_datasets=test_datasets,\n",
        "    prediction_loss_only=False\n",
        ")\n",
        "# Training\n",
        "if training_args.do_train:\n",
        "    model_path = (\n",
        "        model_args.model_name_or_path\n",
        "        if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
        "        else None\n",
        "    )\n",
        "    trainer.train(model_path=model_path)\n",
        "    trainer.save_model()\n",
        "    # For convenience, we also re-save the tokenizer to the same directory,\n",
        "    # so that you can share your model easily on huggingface.co/models =)\n",
        "    if trainer.is_world_master():\n",
        "        tokenizer.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": [
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        eval_output = trainer.evaluate()\n",
        "\n",
        "        perplexity = math.exp(eval_output[\"eval_00_valid_loss\"])\n",
        "        eval_output[\"perplexity\"] = perplexity\n",
        "\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        if trainer.is_world_master():\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results *****\")\n",
        "                for key in sorted(eval_output.keys()):\n",
        "                    logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
        "                    writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
        "\n",
        "        results.update(eval_output)\n",
        "\n",
        "    # if training_args.do_predict:\n",
        "    #     for test_dataset in test_datasets:\n",
        "    #         prediction_output = trainer.predict(test_dataset=test_dataset)\n",
        "    #         predictions = np.argmax(prediction_output.predictions, axis=1) \n",
        "    #         label_ids = prediction_output.label_ids\n",
        "    #     if trainer.is_world_master():\n",
        "\n",
        "\n",
        "    #return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "\n",
        "\n",
        "# In this section - we'll compare accuracy, macro-AUC: just average the auc scores and micro-AUC: just look at each individual prediction.\n",
        "# We'll use accuracy \n",
        "\n",
        "'''\n",
        "        def auc_metrics(yhat_raw, y, ymic):\n",
        "            if yhat_raw.shape[0] <= 1:\n",
        "                return\n",
        "            fpr = {}\n",
        "            tpr = {}\n",
        "            roc_auc = {}\n",
        "            #get AUC for each label individually\n",
        "            relevant_labels = []\n",
        "            auc_labels = {}\n",
        "            for i in range(y.shape[1]):\n",
        "                #only if there are true positives for this label\n",
        "                if y[:,i].sum() > 0:\n",
        "                    fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])\n",
        "                    if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
        "                        auc_score = auc(fpr[i], tpr[i])\n",
        "                        if not np.isnan(auc_score): \n",
        "                            auc_labels[\"auc_%d\" % i] = auc_score\n",
        "                            relevant_labels.append(i)\n",
        "\n",
        "            #macro-AUC: just average the auc scores\n",
        "            aucs = []\n",
        "            for i in relevant_labels:\n",
        "                aucs.append(auc_labels['auc_%d' % i])\n",
        "            roc_auc['auc_macro'] = np.mean(aucs)\n",
        "\n",
        "            #micro-AUC: just look at each individual prediction\n",
        "            yhatmic = yhat_raw.ravel()\n",
        "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic) \n",
        "            roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "            return roc_auc\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "The repo that is provided is missing key elements of the code such as - 'dataset.py' and 'tokenier' information. \n",
        "Also when trying to use pretrained 'tokenizer' and 'pretrained' model, it is producing access error (401).\n",
        "In the original papaer it emphasizes and uses use cases for 'Pancreatic Cancer' and international 'PTSD'. \n",
        "The dataset that we are using has less samples. Due to that reason we think result of the paper is not reproducible.\n",
        "\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmVuzQ724HbO"
      },
      "source": [
        "# Feel free to add new sections"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
